# 📘 Chapter 5: Context-Free Languages

<div align="center">
<h3>Theory of Computation: A Comprehensive Guide</h3>
<p><em>Document Version: 1.0.0 | Last Updated: 2025-07-18 13:12:02 UTC</em></p>
<p><em>Author: AshwinRenjith</em></p>
</div>

---

## 🎯 Chapter Overview

Welcome to Chapter 5, where we'll explore context-free languages—the next level in the Chomsky Hierarchy beyond regular languages. Context-free languages provide the power to describe nested and recursive structures that regular languages cannot handle, making them crucial for understanding programming languages, natural language syntax, and many other hierarchical patterns.

This chapter is designed to make context-free languages accessible to second-year computer engineering students with no prior background. We'll build concepts progressively with intuitive examples to help you understand these important concepts.

Context-free languages are fundamental in computer science with applications in:
- Programming language design and compilation
- Natural language processing
- Data serialization formats (XML, JSON)
- Formal verification
- Pattern recognition in structured data

Let's begin our journey into this fascinating class of languages!

---

## 5.1 Context-Free Grammars (CFGs)

### 📌 5.1.1 Introduction to Context-Free Grammars

**Context-Free Grammars (CFGs)** are formal systems designed to describe the syntax of languages with recursive structures. They extend regular expressions by adding the power to generate nested patterns and balanced structures, which are essential for describing programming languages, natural language syntax, and other hierarchical patterns.

The term "context-free" refers to the fact that variables (non-terminals) can be replaced regardless of the surrounding context—only the variable itself determines what it can be replaced with.

### 📌 5.1.2 Formal Definition of CFGs

A **Context-Free Grammar (CFG)** G is formally defined as a 4-tuple G = (V, Σ, R, S) where:

1. **V**: A finite set of variables or non-terminal symbols
2. **Σ**: A finite set of terminal symbols (the alphabet of the language)
3. **R**: A finite set of rules or productions of the form A → α, where:
   - A ∈ V (a single non-terminal)
   - α ∈ (V ∪ Σ)* (a string of terminals and non-terminals)
4. **S**: The start symbol or start variable (S ∈ V)

The key characteristic of CFGs is that each production rule has exactly one non-terminal symbol on the left-hand side.

### 📌 5.1.3 Grammar Notation

CFGs are typically written using the following conventions:

1. **Variables (non-terminals)** are denoted by uppercase letters (A, B, C, ...) or descriptive names in angle brackets (<expr>, <stmt>, ...)
2. **Terminal symbols** are denoted by lowercase letters (a, b, c, ...) or specific symbols ('if', '+', ')', ...)
3. **The start symbol** is usually denoted by S
4. **Production rules** are written as A → α, where A is a non-terminal and α is a string of terminals and non-terminals
5. **Multiple productions** for the same non-terminal can be combined using the | symbol: A → α₁ | α₂ | ... | αₙ
6. **The empty string** is denoted by ε

### 📌 5.1.4 Examples of Context-Free Grammars

Let's look at some examples of CFGs and the languages they generate:

**Example 1**: A grammar for the language {aⁿbⁿ | n ≥ 1}
```
S → aS b | ab
```

This simple grammar generates strings like "ab", "aabb", "aaabbb", etc., all with an equal number of a's followed by an equal number of b's.

**Example 2**: A grammar for well-balanced parentheses
```
S → (S) | SS | ε
```

This grammar generates strings like "()", "(())", "()()", "((()))()", etc., all with properly matched parentheses.

**Example 3**: A grammar for simple arithmetic expressions
```
E → E + T | E - T | T
T → T * F | T / F | F
F → (E) | id
```

This grammar generates expressions like "id + id * id", "(id + id) / id", etc.

**Example 4**: A grammar for palindromes over {a, b}
```
S → aSa | bSb | a | b | ε
```

This grammar generates strings that read the same forward and backward, like "aba", "abba", "baab", etc.

### 📌 5.1.5 The Language of a CFG

The **language generated by a CFG** G, denoted L(G), is the set of all strings of terminals that can be derived from the start symbol S using the production rules of G.

Formally, w ∈ L(G) if and only if S ⇒* w, where ⇒* represents a sequence of zero or more derivation steps.

A language L is a **context-free language (CFL)** if there exists a CFG G such that L = L(G).

### 📌 5.1.6 CFG Design Techniques

When designing a CFG for a specific language, consider these techniques:

1. **Identify recursive patterns**: Look for nested or repetitive structures in the language
   - Example: For balanced parentheses, notice the recursive pattern of nesting

2. **Break down complex structures**: Decompose the language into simpler components
   - Example: For arithmetic expressions, separate them into expressions, terms, and factors

3. **Handle alternatives**: Use the | operator to specify different possibilities
   - Example: For statements in a programming language, handle different statement types

4. **Ensure coverage**: Make sure the grammar generates all valid strings in the language
   - Example: For a programming language, check that all valid program structures can be generated

5. **Avoid ambiguity** (when possible): Design the grammar to have a unique parse for each string
   - Example: Use precedence rules in expression grammars to avoid ambiguity

### 📌 5.1.7 Common Types of CFGs

Several special types of CFGs are worth noting:

1. **Regular grammar**: A grammar where all productions are of the form A → aB, A → a, or A → ε
   - These generate exactly the regular languages

2. **Linear grammar**: A grammar where each right-hand side has at most one non-terminal
   - Right-linear: A → αB or A → α (non-terminal at the end if present)
   - Left-linear: A → Bα or A → α (non-terminal at the beginning if present)

3. **Ambiguous grammar**: A grammar where some string in the language has more than one leftmost or rightmost derivation
   - Example: The expression grammar with E → E + E | E * E | id is ambiguous

4. **Unambiguous grammar**: A grammar where every string in the language has exactly one leftmost and one rightmost derivation
   - Often preferred for parsing in programming languages

### 📌 5.1.8 Context-Free vs. Regular Languages

Context-free languages properly contain the regular languages:
- Every regular language is context-free
- Some context-free languages are not regular

Key examples of context-free but non-regular languages:
- {aⁿbⁿ | n ≥ 0} (equal numbers of a's and b's)
- {ww^R | w ∈ {a,b}*} (palindromes)
- {w ∈ {a,b}* | w has equal numbers of a's and b's}

These languages require the power of a stack to recognize, which regular languages (finite automata) lack.

### 📌 5.1.9 Applications of CFGs

Context-free grammars are widely used in computer science:

1. **Programming language syntax**: Defining the structure of valid programs
   - Example: The grammar for Java or Python syntax

2. **Parsing**: Converting linear text into structured representations
   - Example: Building an abstract syntax tree from source code

3. **Natural language processing**: Modeling syntactic structures in human languages
   - Example: Phrase structure grammars in linguistics

4. **Data format description**: Specifying the syntax of structured data formats
   - Example: The grammar for XML or JSON

5. **Protocol specification**: Defining communication protocols and message formats
   - Example: The grammar for HTTP headers or network packets

---

## 5.2 Derivations and Parse Trees

### 📌 5.2.1 Introduction to Derivations

A **derivation** is a sequence of steps that transforms the start symbol of a grammar into a specific string in the language. Each step involves replacing a non-terminal symbol with the right-hand side of one of its production rules.

Formally, we say that α directly derives β (written as α ⇒ β) if α = γAδ, β = γωδ, and A → ω is a production in the grammar. In other words, we replace one occurrence of the non-terminal A in α with the string ω.

We use ⇒* to denote a sequence of zero or more derivation steps.

### 📌 5.2.2 Types of Derivations

There are two main types of derivations:

1. **Leftmost derivation**: In each step, replace the leftmost non-terminal
   - Denoted as α ⇒ₗₘ β
   - Used in top-down parsing algorithms

2. **Rightmost derivation**: In each step, replace the rightmost non-terminal
   - Denoted as α ⇒ᵣₘ β
   - Used in bottom-up parsing algorithms

Both leftmost and rightmost derivations generate the same strings, but the sequence of intermediate forms differs.

### 📌 5.2.3 Example of Derivations

Consider the grammar:
```
S → S + S | S * S | a
```

For the string "a + a * a", a leftmost derivation might be:
```
S ⇒ S + S             (using S → S + S)
  ⇒ a + S             (using S → a)
  ⇒ a + S * S         (using S → S * S)
  ⇒ a + a * S         (using S → a)
  ⇒ a + a * a         (using S → a)
```

A rightmost derivation for the same string:
```
S ⇒ S + S             (using S → S + S)
  ⇒ S + S * S         (using S → S * S)
  ⇒ S + S * a         (using S → a)
  ⇒ S + a * a         (using S → a)
  ⇒ a + a * a         (using S → a)
```

### 📌 5.2.4 Parse Trees

A **parse tree** (or syntax tree) is a graphical representation of a derivation that shows the hierarchical structure of the derived string. In a parse tree:

- The root node is the start symbol
- Internal nodes are non-terminals
- Leaf nodes are terminals or ε
- Reading the leaves from left to right gives the derived string

Parse trees abstract away from the specific order of derivation steps, focusing instead on the structure of the string according to the grammar.

### 📌 5.2.5 Example of Parse Trees

For the grammar:
```
S → S + S | S * S | a
```

A parse tree for "a + a * a" could be:

<div align="center">
<pre>
       S
      /|\
     / | \
    S  +  S
    |    /|\
    |   / | \
    a  S  *  S
       |     |
       a     a
</pre>
</div>

Another valid parse tree for the same string:

<div align="center">
<pre>
       S
      /|\
     / | \
    /  |  \
   /   |   \
  S    +    S
  |        /|\
  |       / | \
  a      S  *  S
         |     |
         a     a
</pre>
</div>

The existence of multiple parse trees for the same string indicates that this grammar is **ambiguous**.

### 📌 5.2.6 Ambiguity in Context-Free Grammars

A context-free grammar is **ambiguous** if there exists at least one string in its language that has more than one different parse tree (or, equivalently, more than one leftmost or rightmost derivation).

Ambiguity is often undesirable in programming languages because it can lead to multiple interpretations of the same code. For example, in expression evaluation, ambiguity can lead to different evaluation orders and thus different results.

Classic examples of ambiguity:

1. **Dangling else problem**:
   ```
   S → if E then S | if E then S else S | other
   ```
   The string "if E₁ then if E₂ then S₁ else S₂" can be interpreted in two ways:
   - "if E₁ then (if E₂ then S₁ else S₂)"
   - "if E₁ then (if E₂ then S₁) else S₂"

2. **Expression grammar**:
   ```
   E → E + E | E * E | id
   ```
   The string "id + id * id" can be interpreted as "(id + id) * id" or "id + (id * id)"

### 📌 5.2.7 Resolving Ambiguity

There are several techniques to resolve ambiguity in CFGs:

1. **Grammar restructuring**: Rewrite the grammar to enforce a specific structure
   ```
   E → E + T | T        (addition has lower precedence)
   T → T * F | F        (multiplication has higher precedence)
   F → id | (E)         (identifiers and parenthesized expressions)
   ```

2. **Precedence rules**: Define explicit precedence for operators outside the grammar
   - Often used in parser generators like Yacc or ANTLR

3. **Associativity rules**: Specify how to handle consecutive operations with the same precedence
   - Left-associative: a + b + c = (a + b) + c
   - Right-associative: a = b = c = a = (b = (c))

### 📌 5.2.8 Inherent Ambiguity

Some context-free languages are **inherently ambiguous**, meaning that any CFG that generates them will be ambiguous. No amount of grammar restructuring can make these languages unambiguous.

A famous example is the language L = {aⁿbᵐcⁿdᵐ | n, m ≥ 1} ∪ {aⁿbᵐcᵐdⁿ | n, m ≥ 1}, which contains strings where either the a's match the c's and the b's match the d's, or the a's match the d's and the b's match the c's.

### 📌 5.2.9 Derivation Strategies for Parsing

Different parsing algorithms use different derivation strategies:

1. **Top-down parsing**: Starts from the start symbol and attempts to find a leftmost derivation
   - Examples: Recursive descent parsing, LL parsing
   - Works well with grammars that have specific properties (e.g., LL(1) grammars)

2. **Bottom-up parsing**: Starts from the input string and attempts to find a rightmost derivation in reverse
   - Examples: Shift-reduce parsing, LR parsing
   - Can handle a wider class of grammars than top-down parsing

The choice between these strategies depends on factors like grammar complexity, parsing efficiency, error recovery capabilities, and implementation complexity.

---

## 5.3 Chomsky Normal Form and Greibach Normal Form

### 📌 5.3.1 Introduction to Grammar Normal Forms

**Grammar normal forms** are standardized formats for context-free grammars that simplify theoretical analysis and algorithm development. By transforming any CFG into a normal form, we can develop algorithms that work with a more restricted and predictable grammar structure.

The two most important normal forms are:
1. **Chomsky Normal Form (CNF)**
2. **Greibach Normal Form (GNF)**

Both preserve the language generated by the original grammar (except possibly for the empty string) while enforcing specific restrictions on the format of production rules.

### 📌 5.3.2 Chomsky Normal Form (CNF)

A context-free grammar is in **Chomsky Normal Form** if every production is of the form:

1. A → BC (where A, B, and C are non-terminals)
2. A → a (where A is a non-terminal and a is a terminal)
3. S → ε (where S is the start symbol, only if ε is in the language)

Additionally, the start symbol S does not appear on the right-hand side of any production.

CNF ensures that every non-leaf node in a parse tree has exactly two children (except possibly for a direct derivation to ε from the start symbol).

### 📌 5.3.3 Converting to Chomsky Normal Form

Any context-free grammar can be converted to an equivalent grammar in Chomsky Normal Form. The process involves the following steps:

1. **Add a new start symbol**: Add S₀ → S where S₀ is the new start symbol
   - This ensures the start symbol doesn't appear on the right side of any production

2. **Eliminate ε-productions**: Remove productions of the form A → ε
   - For each production containing A, add a new production with A removed
   - Special case: Keep S → ε if ε is in the language

3. **Eliminate unit productions**: Remove productions of the form A → B where B is a non-terminal
   - Replace A → B with all productions A → γ where B → γ is a production

4. **Convert long productions**: Replace productions with more than two symbols on the right side
   - For A → X₁X₂...Xₙ where n > 2, introduce new non-terminals:
   - A → X₁Y₁, Y₁ → X₂Y₂, ..., Yₙ₋₂ → Xₙ₋₁Xₙ

5. **Handle terminal symbols**: Replace terminals in productions with more than one symbol on the right side
   - For each terminal a in such productions, introduce a new non-terminal Tₐ with Tₐ → a
   - Replace a with Tₐ in the original production

### 📌 5.3.4 Example of CNF Conversion

Let's convert the grammar:
```
S → AB | ε
A → aA | ε
B → bB | b
```

**Step 1**: Add new start symbol
```
S₀ → S
S → AB | ε
A → aA | ε
B → bB | b
```

**Step 2**: Eliminate ε-productions
```
S₀ → S | ε          (since S → ε)
S → AB | A | B      (since A → ε and B appears with A)
A → aA | a          (since A → ε and a appears with A)
B → bB | b
```

**Step 3**: Eliminate unit productions
```
S₀ → AB | A | B | ε
S → AB | aA | a | bB | b
A → aA | a
B → bB | b
```

**Step 4 & 5**: Already in proper format (no long productions or terminals mixed with non-terminals)

Final grammar in CNF:
```
S₀ → AB | A | B | ε
S → AB | aA | a | bB | b
A → aA | a
B → bB | b
```

### 📌 5.3.5 Applications of Chomsky Normal Form

CNF is useful for several applications:

1. **The CYK algorithm**: An efficient dynamic programming algorithm for parsing that requires the grammar to be in CNF
2. **Theoretical proofs**: Many theorems about CFLs are easier to prove when working with CNF
3. **Language recognition**: Some algorithms for CFL recognition are simpler with CNF
4. **Grammar analysis**: CNF makes certain grammar properties easier to analyze

### 📌 5.3.6 Greibach Normal Form (GNF)

A context-free grammar is in **Greibach Normal Form** if every production is of the form:

1. A → aα (where A is a non-terminal, a is a terminal, and α is a string of non-terminals)
2. S → ε (where S is the start symbol, only if ε is in the language)

GNF ensures that every production starts with a terminal followed by zero or more non-terminals. This property is particularly useful for certain types of parsers and for establishing the relationship between CFGs and pushdown automata.

### 📌 5.3.7 Converting to Greibach Normal Form

The conversion to GNF is more complex than to CNF. The main steps are:

1. **Eliminate left recursion**: Replace productions of the form A → Aα | β with:
   - A → βA'
   - A' → αA' | ε

2. **Order the non-terminals**: Assign an order to the non-terminals (e.g., A₁, A₂, ..., Aₙ)

3. **Eliminate non-terminals at the beginning**: For each production Aᵢ → Aⱼα where j < i, replace it with all productions derived from Aⱼ

4. **Handle non-terminals at the beginning**: Replace productions A → Bα where B is a non-terminal with productions A → βα for each B → β

### 📌 5.3.8 Example of GNF Conversion

Let's convert a simple grammar to GNF:
```
S → AB | a
A → BC | b
B → AC | c
C → d
```

**Step 1**: Order non-terminals S, A, B, C

**Step 2**: Eliminate non-terminals at the beginning for S:
```
S → AB | a
```
S doesn't start with a lower-ordered non-terminal, so no change.

**Step 3**: Handle A:
```
S → BC·B | a     (replacing A with BC from A → BC)
S → b·B | a      (replacing A with b from A → b)
A → BC | b
B → AC | c
C → d
```

**Step 4**: Handle B:
```
S → AC·CB | cB | b·B | a     (replacing B with AC or c)
A → AC·C | cC | b             (replacing B with AC or c)
B → AC | c
C → d
```

**Step 5**: Handle C:
```
S → Ad·CB | cB | bd | a
A → Ad·C | cC | b
B → Ad·C | c
C → d
```

**Step 6**: Substitute for remaining non-terminals:
```
S → Ad·CB | cB | bd | a
A → Ad·C | cC | b
B → Ad·C | c
C → d
```

This isn't quite GNF yet, as we need to continue expanding non-terminals at the beginnings of strings. The complete conversion is quite involved, so I've shown just the initial steps of the process.

### 📌 5.3.9 Applications of Greibach Normal Form

GNF has several important applications:

1. **Proving the equivalence** of CFGs and pushdown automata
2. **Developing parsing algorithms** that read input symbols in a left-to-right manner
3. **Simplifying certain theoretical proofs** about context-free languages
4. **Converting CFGs to other computational models** like recursive transition networks

While GNF is theoretically important, CNF is often more practical for applications like efficient parsing algorithms.

---

## 5.4 Pushdown Automata (PDAs)

### 📌 5.4.1 Introduction to Pushdown Automata

**Pushdown Automata (PDAs)** are computational models that extend finite automata with an additional memory component called a stack. This stack allows PDAs to recognize context-free languages, which require more memory than the fixed state-based memory of finite automata.

The key features of a PDA include:
- A finite set of states
- An input alphabet
- A stack alphabet
- A transition function that depends on the current state, the current input symbol (or ε), and the top stack symbol
- The ability to push multiple symbols onto or pop symbols from the stack in a single move

PDAs can recognize languages like {aⁿbⁿ | n ≥ 0} by using the stack to "count" matching symbols—something finite automata cannot do.

### 📌 5.4.2 Formal Definition of a PDA

A **Pushdown Automaton (PDA)** is formally defined as a 7-tuple P = (Q, Σ, Γ, δ, q₀, Z₀, F) where:

1. **Q**: A finite set of states
2. **Σ**: A finite input alphabet
3. **Γ**: A finite stack alphabet
4. **δ**: A transition function mapping Q × (Σ ∪ {ε}) × Γ to finite subsets of Q × Γ*
5. **q₀**: The initial state (q₀ ∈ Q)
6. **Z₀**: The initial stack symbol (Z₀ ∈ Γ)
7. **F**: A set of accepting states (F ⊆ Q)

The transition function δ(q, a, Z) = {(p₁, γ₁), (p₂, γ₂), ..., (pₖ, γₖ)} specifies that if the PDA is in state q, with input symbol a (or ε), and top stack symbol Z, it can move to state pᵢ and replace Z with string γᵢ on the stack.

### 📌 5.4.3 PDA Configurations

A **configuration** of a PDA is a snapshot of its state at a particular moment, represented as a triple (q, w, α) where:
- q is the current state
- w is the remaining input string
- α is the current stack content (with the top of the stack on the left)

The initial configuration is (q₀, w, Z₀), where w is the input string.

### 📌 5.4.4 Moves of a PDA

There are two types of moves (transitions) a PDA can make:

1. **Symbol move**: Reads an input symbol and possibly modifies the stack
   - (q, aw, Zα) ⊢ (p, w, βα) if (p, β) ∈ δ(q, a, Z)

2. **ε-move**: Doesn't read any input but possibly modifies the stack
   - (q, w, Zα) ⊢ (p, w, βα) if (p, β) ∈ δ(q, ε, Z)

We use ⊢* to denote zero or more moves.

### 📌 5.4.5 Language Acceptance by PDAs

There are two ways to define the language accepted by a PDA:

1. **Acceptance by final state**: A string w is accepted if there is a sequence of moves from the initial configuration to a configuration with an accepting state:
   - L(P) = {w | (q₀, w, Z₀) ⊢* (q, ε, α) for some q ∈ F and any stack content α}

2. **Acceptance by empty stack**: A string w is accepted if there is a sequence of moves from the initial configuration to a configuration with an empty stack:
   - L(P) = {w | (q₀, w, Z₀) ⊢* (q, ε, ε) for any state q}

It can be proven that these two acceptance methods are equivalent in power—any language that can be accepted by one method can also be accepted by the other.

### 📌 5.4.6 Example: PDA for {aⁿbⁿ | n ≥ 0}

Let's construct a PDA for the language {aⁿbⁿ | n ≥ 0}:

P = (Q, Σ, Γ, δ, q₀, Z₀, F) where:
- Q = {q₀, q₁, q₂}
- Σ = {a, b}
- Γ = {Z₀, X}
- q₀ is the initial state
- Z₀ is the initial stack symbol
- F = {q₂}

The transition function δ is defined as:
- δ(q₀, ε, Z₀) = {(q₁, Z₀)}            (move to q₁ without reading input)
- δ(q₁, a, Z₀) = {(q₁, XZ₀)}           (on input a, push X onto stack)
- δ(q₁, a, X) = {(q₁, XX)}              (on input a, push X onto stack)
- δ(q₁, b, X) = {(q₁, ε)}               (on input b, pop X from stack)
- δ(q₁, ε, Z₀) = {(q₂, Z₀)}            (when done, move to accepting state)

<div align="center">
<pre>
PDA for {aⁿbⁿ | n ≥ 0}:

              ε, Z₀ → Z₀
    ┌─────┐     ┌─────┐     ┌─────┐
 ───►│ q₀  │---->│ q₁  │---->│ q₂  │
    └─────┘     └─────┘     └─────┘
                   │
                   │ a, Z₀ → XZ₀
                   │ a, X → XX
                   │ b, X → ε
                   │ ε, Z₀ → Z₀
                   ▼
</pre>
</div>

This PDA works by:
1. Pushing an X onto the stack for each 'a' read
2. Popping an X from the stack for each 'b' read
3. Accepting if all X's are popped and the final stack has just Z₀

### 📌 5.4.7 Nondeterminism in PDAs

PDAs, like NFAs, can be nondeterministic. At any point, a PDA may have multiple possible moves for the same input symbol and stack top. This nondeterminism is crucial for the power of PDAs to recognize all context-free languages.

Unlike NFAs, nondeterminism adds essential power to PDAs. **Deterministic PDAs (DPDAs)** are strictly less powerful than nondeterministic PDAs—they can recognize only a proper subset of the context-free languages called **deterministic context-free languages**.

### 📌 5.4.8 PDA Design Techniques

When designing a PDA for a specific language, consider these techniques:

1. **Use the stack to count**: Push symbols on the stack for certain patterns and pop them for matching patterns
   - Example: Push for 'a's and pop for 'b's in {aⁿbⁿ}

2. **Use the stack to remember structure**: Push symbols to track nested patterns
   - Example: Push for '(' and pop for ')' in balanced parentheses

3. **Use multiple stack symbols**: Different stack symbols can track different types of patterns
   - Example: Use different symbols for different types of parentheses

4. **Use states to track phases**: Different states can represent different phases of recognition
   - Example: Use one state for reading 'a's and another for reading 'b's

5. **Use ε-transitions strategically**: Move between states without consuming input when needed
   - Example: Use ε-transitions to handle optional parts of the language

### 📌 5.4.9 Limitations of PDAs

While PDAs are more powerful than finite automata, they still have limitations:

1. **Cannot count multiple patterns**: A single stack cannot track multiple independent counts
   - Example: PDAs cannot recognize {aⁿbⁿcⁿ | n ≥ 0}

2. **Cannot match nested cross-dependencies**: PDAs cannot recognize patterns like {aᵐbⁿcᵐdⁿ | m, n ≥ 0}
   - This would require keeping track of two independent nested patterns

3. **Stack operations are restricted**: PDAs can only access the top of the stack, unlike Turing machines which have unrestricted memory access

These limitations define the boundary between context-free languages and more complex language classes in the Chomsky Hierarchy.

---

## 5.5 Equivalence of PDAs and CFGs

### 📌 5.5.1 The Relationship Between PDAs and CFGs

One of the most important results in the theory of computation is that pushdown automata and context-free grammars are equivalent in power—they describe exactly the same class of languages:

1. For every context-free grammar G, there exists a PDA P such that L(G) = L(P).
2. For every PDA P, there exists a context-free grammar G such that L(P) = L(G).

This equivalence provides two complementary ways to describe context-free languages: grammars focus on generation, while automata focus on recognition.

### 📌 5.5.2 Converting CFGs to PDAs

To convert a context-free grammar G = (V, Σ, R, S) to an equivalent PDA P:

1. Construct a PDA P = (Q, Σ, Γ, δ, q₀, Z₀, F) where:
   - Q = {q₀, q₁, q₂} (three states)
   - Γ = V ∪ Σ ∪ {Z₀} (stack alphabet includes variables, terminals, and the start stack symbol)
   - F = {q₂} (a single accepting state)

2. Define the transition function δ:
   - δ(q₀, ε, Z₀) = {(q₁, SZ₀)} (push the grammar's start symbol onto the stack)
   - For each variable A and production A → α:
     - δ(q₁, ε, A) = {(q₁, α)} (replace A with α on the stack)
   - For each terminal a:
     - δ(q₁, a, a) = {(q₁, ε)} (match and consume a terminal on input and stack)
   - δ(q₁, ε, Z₀) = {(q₂, Z₀)} (move to accepting state when done)

This PDA simulates leftmost derivations of the grammar. It works by:
- Initializing the stack with the start symbol
- Repeatedly replacing variables on the stack with their production right-hand sides
- Matching terminals on the stack with input symbols
- Accepting when the stack is reduced to just Z₀

<div align="center">
<pre>
Generic PDA for any CFG:

                 ε, Z₀ → SZ₀
    ┌─────┐     ┌─────┐     ┌─────┐
 ───►│ q₀  │---->│ q₁  │---->│ q₂  │
    └─────┘     └─────┘     └─────┘
                   │
                   │ ε, A → α (for A → α in G)
                   │ a, a → ε (for a ∈ Σ)
                   │ ε, Z₀ → Z₀
                   ▼
</pre>
</div>

### 📌 5.5.3 Example: Converting a CFG to a PDA

Let's convert the grammar G with productions:
```
S → aSb | ε
```

We construct a PDA P = (Q, Σ, Γ, δ, q₀, Z₀, F) where:
- Q = {q₀, q₁, q₂}
- Σ = {a, b}
- Γ = {S, a, b, Z₀}
- F = {q₂}

The transition function δ is:
- δ(q₀, ε, Z₀) = {(q₁, SZ₀)}
- δ(q₁, ε, S) = {(q₁, aSb), (q₁, ε)}
- δ(q₁, a, a) = {(q₁, ε)}
- δ(q₁, b, b) = {(q₁, ε)}
- δ(q₁, ε, Z₀) = {(q₂, Z₀)}

This PDA recognizes the language {aⁿbⁿ | n ≥ 0}.

### 📌 5.5.4 Converting PDAs to CFGs

Converting a PDA to a CFG is more complex. The high-level approach is:

1. Construct a grammar G = (V, Σ, R, S) where:
   - Variables in V are of the form [qAp], representing that the PDA can go from state q to state p and remove A from the stack
   - The start symbol S is [q₀Z₀qₐ] for some accepting state qₐ

2. Add productions to G:
   - For each transition δ(q, a, A) = {(p, ε)} (pop A):
     - Add [qAp] → a
   - For each transition δ(q, a, A) = {(r, BC)} (replace A with BC):
     - Add [qAp] → a[rBs][sCp] for all states s
   - For each transition δ(q, a, A) = {(r, B₁B₂...Bₙ)} (n ≥ 3):
     - Add similar rules with multiple intermediate states
   - Add [qAq] → ε for all states q and stack symbols A

The resulting grammar may be large and contain many productions, but it will generate exactly the language accepted by the PDA.

### 📌 5.5.5 Example: Converting a Simple PDA to a CFG

Let's convert a simple PDA that recognizes {aⁿbⁿ | n ≥ 1}.

PDA transitions:
- δ(q₀, a, Z₀) = {(q₁, XZ₀)}
- δ(q₁, a, X) = {(q₁, XX)}
- δ(q₁, b, X) = {(q₂, ε)}
- δ(q₂, b, X) = {(q₂, ε)}
- δ(q₂, ε, Z₀) = {(q₃, ε)}
- F = {q₃}

Resulting CFG:
- S → [q₀Z₀q₃]
- [q₀Z₀q₃] → a[q₁XZ₀q₃]
- [q₁XZ₀q₃] → a[q₁XX₁][q₁X₁q₃] | b[q₂Z₀q₃]
- [q₁XX₁] → a[q₁XX₂][q₁X₂X₁] | b[q₂X₁]
- [q₂Z₀q₃] → ε
- [q₂X₁] → b
- ...

This is a simplified version of the conversion; the complete grammar would have more productions.

### 📌 5.5.6 Implications of the Equivalence

The equivalence of PDAs and CFGs has several important implications:

1. **Dual perspective**: It provides complementary ways to think about context-free languages—generative (grammars) and recognitive (automata)

2. **Algorithm design**: Some problems are easier to solve using one model than the other
   - Parsing is often easier to understand in terms of grammars
   - Recognition algorithms are sometimes clearer in terms of PDAs

3. **Theoretical unification**: It establishes context-free languages as a robust class defined by multiple equivalent models

4. **Language properties**: Properties proven for one model automatically apply to the other
   - For example, the pumping lemma for CFLs can be derived using either model

### 📌 5.5.7 Deterministic PDAs and Deterministic CFLs

While nondeterministic PDAs are equivalent to CFGs, deterministic PDAs (DPDAs) are less powerful. They recognize a proper subset of CFLs called **deterministic context-free languages (DCFLs)**.

DPDAs are important in practice because they allow for efficient parsing algorithms. Many programming languages are designed to be deterministic context-free languages to enable efficient compilation.

Examples of languages recognized by DPDAs:
- {aⁿbⁿ | n ≥ 0} (deterministic context-free)
- {ww^R | w ∈ {a,b}*} (palindromes, non-deterministic context-free)
- {a^ib^jc^k | i = j or j = k} (inherently ambiguous, non-deterministic context-free)

The relationship between language classes can be summarized as:
Regular Languages ⊂ Deterministic CFLs ⊂ Context-Free Languages

---

## 5.6 Properties of Context-Free Languages

### 📌 5.6.1 Closure Properties of CFLs

Context-free languages are closed under certain operations, meaning that when we apply these operations to CFLs, the result is always a CFL. Understanding these properties helps in constructing and analyzing complex context-free languages.

**Closure properties of context-free languages:**

1. **Union**: If L₁ and L₂ are CFLs, then L₁ ∪ L₂ is a CFL.
   - Proof idea: Create a new grammar with start symbol S and productions S → S₁ | S₂, where S₁ and S₂ are the start symbols of the grammars for L₁ and L₂.

2. **Concatenation**: If L₁ and L₂ are CFLs, then L₁L₂ is a CFL.
   - Proof idea: Create a new grammar with start symbol S and production S → S₁S₂, where S₁ and S₂ are the start symbols of the grammars for L₁ and L₂.

3. **Kleene star**: If L is a CFL, then L* is a CFL.
   - Proof idea: Create a new grammar with start symbol S and productions S → SS | S₁ | ε, where S₁ is the start symbol of the grammar for L.

4. **Substitution**: If L is a CFL and for each symbol a in the alphabet, a is replaced by some CFL Lₐ, then the resulting language is a CFL.
   - This is a generalization of the above closure properties.

5. **Homomorphism**: If L is a CFL and h is a homomorphism, then h(L) is a CFL.
   - Proof idea: Replace each terminal a in the grammar for L with the string h(a).

6. **Inverse homomorphism**: If L is a CFL and h is a homomorphism, then h⁻¹(L) is a CFL.
   - Proof idea: Construct a PDA for L and modify it to simulate the homomorphism in reverse.

7. **Intersection with regular languages**: If L is a CFL and R is a regular language, then L ∩ R is a CFL.
   - Proof idea: Construct a PDA for L and a DFA for R, then create a product construction that simulates both machines in parallel.

### 📌 5.6.2 Non-closure Properties of CFLs

Context-free languages are **not closed** under certain operations:

1. **Intersection**: The intersection of two CFLs is not necessarily a CFL.
   - Counterexample: L₁ = {aⁿbⁿcᵐ | n, m ≥ 0} and L₂ = {aᵐbⁿcⁿ | n, m ≥ 0} are both CFLs, but L₁ ∩ L₂ = {aⁿbⁿcⁿ | n ≥ 0} is not a CFL.

2. **Complement**: The complement of a CFL is not necessarily a CFL.
   - Counterexample: {ww | w ∈ {a,b}*} is not a CFL, but its complement is not a CFL either.

3. **Difference**: The difference of two CFLs is not necessarily a CFL.
   - This follows from the non-closure under intersection and complement.

4. **Set operations involving DCFLs**: While DCFLs are closed under complement, they are not closed under union, concatenation, or Kleene star.

### 📌 5.6.3 Context-Free Pumping Lemma (Overview)

Like regular languages, context-free languages have a pumping property that is useful for proving that certain languages are not context-free. We'll explore this in detail in section 5.7, but here's a preview:

For any context-free language L, there exists a pumping length p such that any string s in L with |s| ≥ p can be written as s = uvwxy where:
1. |vwx| ≤ p
2. |vx| ≥ 1
3. For all i ≥ 0, uv^i wx^i y is in L

This lemma helps prove that languages like {aⁿbⁿcⁿ | n ≥ 0} are not context-free.

### 📌 5.6.4 Deterministic Context-Free Languages (DCFLs)

A context-free language is **deterministic** if it can be recognized by a deterministic PDA. DCFLs form a proper subset of CFLs and have some distinctive properties:

1. DCFLs are closed under complement (unlike general CFLs).
2. DCFLs are not closed under union, concatenation, or Kleene star.
3. The intersection of two DCFLs is not necessarily a DCFL.
4. Every DCFL has an unambiguous grammar (though not every unambiguous CFL is deterministic).

Deterministic languages are particularly important in compiler design because they can be parsed efficiently in linear time.

### 📌 5.6.5 Inherent Ambiguity

Some context-free languages are **inherently ambiguous**, meaning that any grammar generating them will be ambiguous. No amount of grammar redesign can eliminate the ambiguity.

Famous examples of inherently ambiguous languages:
1. L = {a^i b^j c^k | i = j or j = k, where i, j, k ≥ 1}
2. L = {a^m b^n c^m d^n | m, n ≥ 1} ∪ {a^m b^n c^n d^m | m, n ≥ 1}

The existence of inherently ambiguous languages is a fundamental limitation in the theory of context-free languages and has implications for parsing and language design.

### 📌 5.6.6 Chomsky-Schützenberger Theorem

The **Chomsky-Schützenberger theorem** provides an alternative characterization of context-free languages:

Every context-free language L can be expressed as h(D ∩ R), where:
- D is the language of balanced parentheses (a Dyck language)
- R is a regular language
- h is a homomorphism

This theorem establishes a deep connection between context-free languages and the structure of balanced parentheses, highlighting the fundamental relationship between CFLs and nested structures.

### 📌 5.6.7 Parikh's Theorem

**Parikh's theorem** states that for any context-free language, the set of Parikh vectors (which count the occurrences of each terminal symbol) is a semilinear set.

In simpler terms, if we only care about how many of each symbol appears in a string (not their order), then context-free languages are equivalent to regular languages. This means the "counting power" of CFLs only matters when combined with order constraints.

### 📌 5.6.8 Ogden's Lemma

**Ogden's lemma** is a strengthening of the pumping lemma for context-free languages. It allows for the marking of specific positions in the string and ensures that the pumping involves at least some of these marked positions.

This more powerful lemma can prove that certain languages are not context-free when the standard pumping lemma might not be sufficient.

### 📌 5.6.9 Context-Free Languages and Pattern Matching

Context-free languages excel at matching patterns with hierarchical or nested structure:

1. **Balanced parentheses**: Different types of brackets that must be properly matched
2. **Nested structures**: HTML/XML tags or nested comments in programming languages
3. **Simple recursive patterns**: Expressions with matching start and end markers

They cannot, however, match patterns requiring multiple independent counts or cross-serial dependencies (like {a^m b^n c^m d^n}), which would require more powerful computational models.

---

## 5.7 The Pumping Lemma for Context-Free Languages

### 📌 5.7.1 Motivation and Intuition

Just as the pumping lemma for regular languages provides a tool for proving that certain languages are not regular, the **pumping lemma for context-free languages** helps us prove that certain languages are not context-free.

The intuition behind the lemma is based on the fact that any sufficiently long string in a context-free language must contain a repeated structure in its parse tree. This repetition allows parts of the string to be "pumped" (repeated) while still generating valid strings in the language.

Understanding this lemma is crucial for identifying the boundaries between context-free languages and more complex language classes.

### 📌 5.7.2 Statement of the Pumping Lemma

**The Pumping Lemma for Context-Free Languages**: If L is a context-free language, then there exists a constant p > 0 (called the pumping length) such that any string s ∈ L with |s| ≥ p can be written as s = uvwxy, where:

1. |vwx| ≤ p (the middle part is bounded by p)
2. |vx| ≥ 1 (at least one of v and x is non-empty)
3. For all i ≥ 0, uv^i wx^i y ∈ L (the string remains in the language when v and x are pumped the same number of times)

The key difference from the regular language pumping lemma is that here we can pump two separate parts of the string (v and x) rather than just one contiguous substring.

### 📌 5.7.3 Proof Sketch of the Pumping Lemma

The pumping lemma follows from properties of parse trees for context-free grammars:

1. For any CFG in Chomsky Normal Form, a string of length n has a parse tree of height at least log₂(n) + 1.
2. If the parse tree height exceeds the number of variables in the grammar, then some variable must repeat on a path from the root to a leaf.
3. This repetition creates a "pumpable" structure in the parse tree.
4. Setting the pumping length p to be sufficiently large ensures the existence of such a repetition.

<div align="center">
<pre>
        S
       / \
      /   \
     A     B
    / \   / \
   C   D E   F
  /     / \
 a     b   G
            \
             c

In a tall enough parse tree, some variable must repeat on a path,
creating a structure that can be "pumped":

        S
       / \
      /   \
     A     B
    / \   / \
   C   D E   F
  /     / \
 a     E   G  ← Variable E repeats
      / \   \
     b   G   c
          \
           c
</pre>
</div>

### 📌 5.7.4 Using the Pumping Lemma (Proof by Contradiction)

To prove that a language L is not context-free using the pumping lemma:

1. Assume L is context-free, so the pumping lemma applies with some pumping length p.
2. Select a string s ∈ L where |s| ≥ p, carefully chosen to exploit the structure of L.
3. Consider all possible ways to divide s = uvwxy such that |vwx| ≤ p and |vx| ≥ 1.
4. Show that for at least one value of i ≥ 0 (often i = 0 or i = 2), uv^i wx^i y ∉ L.
5. This contradicts the pumping lemma, proving L is not context-free.

### 📌 5.7.5 Example: Proving {a^n b^n c^n | n ≥ 1} is Not Context-Free

Let's prove that the language L = {a^n b^n c^n | n ≥ 1} is not context-free.

**Step 1**: Assume L is context-free. Then there exists a pumping length p.

**Step 2**: Consider the string s = a^p b^p c^p ∈ L.

**Step 3**: By the pumping lemma, s can be written as uvwxy where:
- |vwx| ≤ p
- |vx| ≥ 1
- For all i ≥ 0, uv^i wx^i y ∈ L

**Step 4**: Since |vwx| ≤ p, vwx must be contained within one of these sections or cross a boundary:
- Case 1: vwx consists only of a's
- Case 2: vwx consists of some a's and some b's
- Case 3: vwx consists only of b's
- Case 4: vwx consists of some b's and some c's
- Case 5: vwx consists only of c's

**Step 5**: In any of these cases, since |vx| ≥ 1, pumping v and x will change the balance of a's, b's, and c's. For instance:
- If vx contains only a's, then uv²wx²y will have more a's than b's or c's
- If vx contains some a's and some b's, then uv²wx²y will have more a's and b's but not more c's
- Similar issues arise in other cases

**Step 6**: Therefore, there exists an i (specifically, i = 2) where uv^i wx^i y ∉ L, contradicting the pumping lemma.

**Conclusion**: The language L = {a^n b^n c^n | n ≥ 1} is not context-free.

### 📌 5.7.6 Example: Proving {w#w | w ∈ {a,b}*} is Not Context-Free

Let's prove that the language L = {w#w | w ∈ {a,b}*} (strings of the form w#w where w is any string of a's and b's) is not context-free.

**Step 1**: Assume L is context-free with pumping length p.

**Step 2**: Consider the string s = a^p b^p # a^p b^p ∈ L.

**Step 3**: By the pumping lemma, s = uvwxy where |vwx| ≤ p, |vx| ≥ 1, and uv^i wx^i y ∈ L for all i ≥ 0.

**Step 4**: Since |vwx| ≤ p, vwx must be contained within one of these sections:
- Case 1: vwx is contained in the first a^p
- Case 2: vwx spans the first a^p and b^p
- Case 3: vwx is contained in the first b^p
- Case 4: vwx spans the first b^p and the #
- Case 5: vwx spans the # and the second a^p
- Case 6: vwx is contained in the second a^p
- Case 7: vwx spans the second a^p and b^p
- Case 8: vwx is contained in the second b^p

**Step 5**: In any of these cases, pumping v and x will create a string that's not of the form w#w:
- If vwx is entirely before or after the #, pumping will create different strings on either side
- If vwx spans the #, pumping will change the position of the # or create multiple #'s

**Step 6**: Therefore, there exists an i (typically i = 0 or i = 2) where uv^i wx^i y ∉ L, contradicting the pumping lemma.

**Conclusion**: The language L = {w#w | w ∈ {a,b}*} is not context-free.

### 📌 5.7.7 Common Mistakes When Using the Pumping Lemma

1. **Incorrect assumption about the division**: The pumping lemma only guarantees that there exists some way to divide the string as uvwxy satisfying the conditions. It doesn't let you choose the specific division.

2. **Not considering all possible divisions**: You must show that no valid division of s = uvwxy can satisfy the pumping property.

3. **Not considering all pumpable strings**: Sometimes you need to carefully choose which string to apply the lemma to, as not all strings in the language may lead to a contradiction.

4. **Insufficient contradiction**: Make sure you clearly demonstrate that the pumped string violates a defining property of the language.

5. **Trying to use the pumping lemma to prove a language is context-free**: The pumping lemma only provides a necessary condition for a language to be context-free, not a sufficient one.

### 📌 5.7.8 Limitations of the Pumping Lemma

The pumping lemma has some limitations:

1. It's a one-way test: It can prove languages are not context-free, but satisfying the lemma doesn't guarantee a language is context-free.

2. Some non-context-free languages satisfy the pumping lemma, such as {a^n b^n c^n d^n | n ≥ 0}.

3. For certain complex languages, the pumping lemma may be difficult to apply, and other techniques like Ogden's lemma may be more effective.

### 📌 5.7.9 Ogden's Lemma: A Stronger Version

**Ogden's lemma** is a strengthening of the pumping lemma that gives more control over which positions in the string are pumped.

The lemma states: If L is a context-free language, then there exists a constant p such that for any string s ∈ L with at least p positions marked, s can be written as uvwxy where:
1. vwx has at most p marked positions
2. vx has at least one marked position
3. For all i ≥ 0, uv^i wx^i y ∈ L

This stronger lemma can prove certain languages are not context-free when the standard pumping lemma is insufficient.

---

## 5.8 Decision Problems for CFLs

### 📌 5.8.1 Introduction to Decision Problems for CFLs

A **decision problem** is a question with a yes-or-no answer. For context-free languages, these problems involve determining whether a CFG or PDA has certain properties or whether two models are equivalent.

Understanding which problems are decidable (can be solved algorithmically) and which are undecidable (no algorithm exists) is crucial for working with context-free languages. The decidability landscape for CFLs differs significantly from that of regular languages, with many more undecidable problems.

### 📌 5.8.2 Decidable Problems for CFLs

The following problems are decidable for context-free languages:

1. **Membership**: Given a string w and a CFG G (or PDA P), is w ∈ L(G)?
   - Algorithm: The CYK algorithm solves this in O(n³) time for a grammar in Chomsky Normal Form, where n is the length of the string.

2. **Emptiness**: Given a CFG G, is L(G) = ∅?
   - Algorithm: Check if any terminal string is derivable from the start symbol using a marking algorithm.

3. **Finiteness**: Given a CFG G, is L(G) finite?
   - Algorithm: Check for cycles in the dependency graph that can generate infinitely many strings.

4. **Generation of ε**: Given a CFG G, is ε ∈ L(G)?
   - Algorithm: Check if the start symbol is nullable (can derive ε).

5. **Generation of a specific string**: Given a CFG G and a string w, does G generate w?
   - Algorithm: Use the CYK algorithm or another parsing algorithm.

### 📌 5.8.3 The CYK Algorithm

The **Cocke-Younger-Kasami (CYK) algorithm** is an efficient dynamic programming algorithm for determining whether a string belongs to a context-free language. It requires the grammar to be in Chomsky Normal Form.

The algorithm works as follows:

1. Let w = a₁a₂...aₙ be the input string of length n.
2. Construct an n×n table T where T[i,j] contains the set of non-terminals that derive the substring a_i...a_{i+j-1}.
3. Fill in the table bottom-up:
   - Base case: T[i,1] = {A | A → aᵢ}
   - Recursive case: T[i,j] = {A | A → BC where B ∈ T[i,k] and C ∈ T[i+k,j-k] for some 1 ≤ k < j}
4. The string w is in the language if and only if the start symbol S is in T[1,n].

<div align="center">
<pre>
CYK Algorithm Example for the string "aabb":

Grammar in CNF:
S → AB | BC
A → BA | a
B → CC | b
C → AB | a

Table T:
    1      2       3       4
1   {A,C}  {S}     {S,C}   {S}
2          {B}     {A}     {S,C}
3                  {A,C}   {S}
4                          {B}

Since S ∈ T[1,4], the string "aabb" is in the language.
</pre>
</div>

The CYK algorithm has time complexity O(n³|G|), where n is the length of the input string and |G| is the size of the grammar.

### 📌 5.8.4 Undecidable Problems for CFLs

Several important problems for context-free languages are undecidable:

1. **Universality**: Given a CFG G, is L(G) = Σ*?
   - No algorithm can determine this for all context-free grammars.

2. **Equivalence**: Given two CFGs G₁ and G₂, is L(G₁) = L(G₂)?
   - This is undecidable even for simple subclasses of CFGs.

3. **Containment**: Given two CFGs G₁ and G₂, is L(G₁) ⊆ L(G₂)?
   - No algorithm can determine this for all context-free grammars.

4. **Intersection emptiness**: Given two CFGs G₁ and G₂, is L(G₁) ∩ L(G₂) = ∅?
   - This is undecidable, reflecting the fact that CFLs are not closed under intersection.

5. **Ambiguity**: Given a CFG G, is G ambiguous?
   - Determining whether a CFG generates some string with multiple parse trees is undecidable.

6. **Regularity**: Given a CFG G, is L(G) a regular language?
   - No algorithm can determine this for all context-free grammars.

These undecidability results highlight the complexity of working with context-free languages compared to regular languages.

### 📌 5.8.5 The Post Correspondence Problem

Many undecidability results for CFLs are proven by reduction from the **Post Correspondence Problem (PCP)**, a famous undecidable problem.

The PCP is defined as follows:
- Given a set of domino pairs (u₁, v₁), (u₂, v₂), ..., (uₙ, vₙ) where uᵢ and vᵢ are strings
- Determine if there exists a sequence of indices i₁, i₂, ..., iₖ such that uᵢ₁uᵢ₂...uᵢₖ = vᵢ₁vᵢ₂...vᵢₖ

This seemingly simple matching problem is undecidable, and many problems about CFLs can be reduced to it, proving their undecidability as well.

### 📌 5.8.6 Decidability for Deterministic CFLs

For **deterministic context-free languages (DCFLs)**, recognized by deterministic PDAs, the decidability landscape is somewhat different:

1. **Equivalence**: Given two DPDAs P₁ and P₂, is L(P₁) = L(P₂)?
   - This is decidable for DCFLs (unlike for general CFLs).

2. **Containment**: Given two DPDAs P₁ and P₂, is L(P₁) ⊆ L(P₂)?
   - This is decidable for DCFLs.

3. **Complement**: Given a DPDA P, finding a DPDA for the complement of L(P) is effective.
   - This is because DCFL